{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3bdcc29",
   "metadata": {},
   "source": [
    "# Dataset Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69405007",
   "metadata": {},
   "source": [
    "Configure the paths and the libraries used for preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e21d3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Adria Espinoza\\FIB\\mds\\Q2\\ML\\ml-project\\src\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2290977",
   "metadata": {},
   "source": [
    "### Data Extraction\n",
    "First we have extracted the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e526942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe columns\n",
      "Index(['language', 'sentence', 'n', 'edgelist', 'root'], dtype='object')\n",
      "Dataframe rows\n",
      "10500\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/raw_files/train.csv\")\n",
    "df['edgelist'] = df['edgelist'].apply(ast.literal_eval) # to verify that we are sending a list instead of an array\n",
    "\n",
    "print(\"Dataframe columns\")\n",
    "print(df.columns)\n",
    "\n",
    "print(\"Dataframe rows\")\n",
    "print(len(df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c26d1f5",
   "metadata": {},
   "source": [
    "### Transforming the dataframe\n",
    "-  To meet the project's requirements, the following code breaks down sentences into finer granularity.\n",
    "- Instead of working with full sentences, we now operate at the level of vertices (as nodes in a sentence treemap)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91fe0204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def centralities(edgelist):\n",
    "    \"\"\"\n",
    "    - edgelist is a list of node pairs e.g. [(7,2),(1,7),(1,9),...]\n",
    "    - returns a dictionary of vertex -> (centrality values)\n",
    "    \"\"\"\n",
    "    T = nx.from_edgelist(edgelist)\n",
    "    dc = nx.degree_centrality(T)\n",
    "    cc = nx.harmonic_centrality(T)\n",
    "    bc = nx.betweenness_centrality(T)\n",
    "    pc = nx.pagerank(T)\n",
    "    return {v: (dc[v], cc[v], bc[v], pc[v]) for v in T}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "205b06d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of         language  sentence   n  vertex    degree  closeness  betweenness  \\\n",
      "0       Japanese         2  23       6  0.090909   5.823846     0.090909   \n",
      "1       Japanese         2  23       4  0.045455   4.561122     0.000000   \n",
      "2       Japanese         2  23       2  0.136364   6.991703     0.255411   \n",
      "3       Japanese         2  23      23  0.045455   5.157179     0.000000   \n",
      "4       Japanese         2  23      20  0.090909   7.146825     0.311688   \n",
      "...          ...       ...  ..     ...       ...        ...          ...   \n",
      "197474   Russian       995  19      19  0.055556   5.005159     0.000000   \n",
      "197475   Russian       995  19       1  0.055556   6.034524     0.000000   \n",
      "197476   Russian       995  19      14  0.055556   6.034524     0.000000   \n",
      "197477   Russian       995  19       5  0.111111   6.701190     0.111111   \n",
      "197478   Russian       995  19      16  0.055556   5.005159     0.000000   \n",
      "\n",
      "        pagerank  is_root  \n",
      "0       0.048565        0  \n",
      "1       0.027162        0  \n",
      "2       0.066901        0  \n",
      "3       0.025477        0  \n",
      "4       0.042552        0  \n",
      "...          ...      ...  \n",
      "197474  0.032147        0  \n",
      "197475  0.029739        0  \n",
      "197476  0.029739        0  \n",
      "197477  0.057065        0  \n",
      "197478  0.032147        0  \n",
      "\n",
      "[197479 rows x 9 columns]>\n"
     ]
    }
   ],
   "source": [
    "# List to collect rows for the final DataFrame\n",
    "rows = []\n",
    "\n",
    "# Iterate over DataFrame rows\n",
    "for idx, row in df.iterrows():\n",
    "    # Compute centralities\n",
    "    centrality_dict = centralities(row['edgelist'])\n",
    "    \n",
    "    # Build a flat row per node\n",
    "    for vertex, (deg, clos, betw, pr) in centrality_dict.items():\n",
    "        rows.append({\n",
    "            'language': row['language'],\n",
    "            'sentence': row['sentence'],\n",
    "            'n': row['n'],\n",
    "            'vertex': vertex,\n",
    "            'degree': deg,\n",
    "            'closeness': clos,\n",
    "            'betweenness': betw,\n",
    "            'pagerank': pr,\n",
    "            'is_root': int(vertex == row['root'])\n",
    "        })\n",
    "\n",
    "# Convert list of rows to a DataFrame\n",
    "df_filtered = pd.DataFrame(rows)\n",
    "\n",
    "print(df_filtered.head)\n",
    "df_filtered.to_csv(\"./data/preprocessed/data_preprocessed.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a0af50",
   "metadata": {},
   "source": [
    "### Outlier Detection\n",
    "We have decided to keep the outliers until it is find a very sensitive to outliers problem.\n",
    "Keeping outliers ensures that the full variability and complexity of the data are preserved, which may reflect meaningful linguistic or structural phenomena. Removing them could eliminate rare but important patterns that are critical for understanding or modeling real-world sentence structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111e2582",
   "metadata": {},
   "source": [
    "### Data normalization\n",
    "\n",
    "Different metrics are normalized at a Language sentence level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0c15211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Min-max normalization function\n",
    "def min_max_normalize(metric_dict):\n",
    "    values = np.array(list(metric_dict.values()), dtype=np.float64)\n",
    "    min_val = np.min(values)\n",
    "    max_val = np.max(values)\n",
    "    if max_val == min_val:\n",
    "        return {k: 0.0 for k in metric_dict}  # Avoid division by zero\n",
    "    return {k: (v - min_val) / (max_val - min_val) for k, v in metric_dict.items()}\n",
    "\n",
    "# Normalize centralities per sentence-language pair\n",
    "def normalize_centralities(df):\n",
    "    required_columns = ['sentence', 'language', 'vertex', 'degree', 'closeness', 'betweenness', 'pagerank', 'is_root']\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        raise ValueError(f\"Missing required columns: {set(required_columns) - set(df.columns)}\")\n",
    "    \n",
    "    metrics = ['degree', 'closeness', 'betweenness', 'pagerank']\n",
    "    result_frames = []\n",
    "\n",
    "    # Group by sentence and language\n",
    "    for (sentence, language), group in df.groupby(['sentence', 'language']):\n",
    "        norm_data = {}\n",
    "\n",
    "        for metric in metrics:\n",
    "            metric_dict = dict(zip(group['vertex'], group[metric]))\n",
    "            norm_data[metric] = min_max_normalize(metric_dict)\n",
    "\n",
    "        # Copy group and apply normalized values\n",
    "        norm_df = group.copy()\n",
    "        for metric in metrics:\n",
    "            norm_df[f'{metric}_norm'] = norm_df['vertex'].map(norm_data[metric])\n",
    "\n",
    "        result_frames.append(norm_df)\n",
    "\n",
    "    # Combine all normalized groups\n",
    "    return pd.concat(result_frames, ignore_index=True)\n",
    "\n",
    "# Normalize length \n",
    "def normalize_length(df):\n",
    "    df['n_norm'] = (df['n'] - df['n'].min()) / (df['n'].max() - df['n'].min())\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply normalization and save\n",
    "df_normalized = normalize_centralities(df_filtered)\n",
    "\n",
    "df_normalized = normalize_length(df_normalized)\n",
    "\n",
    "df_normalized.to_csv(\"./data/preprocessed/data_normalized.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
