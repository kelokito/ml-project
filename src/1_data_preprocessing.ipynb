{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3bdcc29",
   "metadata": {},
   "source": [
    "# Dataset Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69405007",
   "metadata": {},
   "source": [
    "Configure the paths and the libraries used for preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e21d3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Adria Espinoza\\FIB\\mds\\Q2\\ML\\ml-project\\src\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2290977",
   "metadata": {},
   "source": [
    "### Data Extraction\n",
    "First we have extracted the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e526942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe columns\n",
      "Index(['language', 'sentence', 'n', 'edgelist', 'root'], dtype='object')\n",
      "Dataframe rows\n",
      "10500\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/raw_files/train.csv\")\n",
    "df['edgelist'] = df['edgelist'].apply(ast.literal_eval) # to verify that we are sending a list instead of an array\n",
    "\n",
    "print(\"Dataframe columns\")\n",
    "print(df.columns)\n",
    "\n",
    "print(\"Dataframe rows\")\n",
    "print(len(df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e22d7a0",
   "metadata": {},
   "source": [
    "### Function declaration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91fe0204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def centralities(edgelist):\n",
    "    \"\"\"\n",
    "    - edgelist is a list of node pairs e.g. [(7,2),(1,7),(1,9),...]\n",
    "    - returns a dictionary of vertex -> (centrality values)\n",
    "    \"\"\"\n",
    "    T = nx.from_edgelist(edgelist)\n",
    "    dc = nx.degree_centrality(T)\n",
    "    cc = nx.harmonic_centrality(T)\n",
    "    bc = nx.betweenness_centrality(T)\n",
    "    pc = nx.pagerank(T)\n",
    "    return {v: (dc[v], cc[v], bc[v], pc[v]) for v in T}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c26d1f5",
   "metadata": {},
   "source": [
    "### Transforming the dataframe\n",
    "-  To meet the project's requirements, the following code breaks down sentences into finer granularity.\n",
    "- Instead of working with full sentences, we now operate at the level of vertices (as nodes in a sentence treemap)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205b06d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to collect rows for the final DataFrame\n",
    "rows = []\n",
    "\n",
    "# Iterate over DataFrame rows\n",
    "for idx, row in df.iterrows():\n",
    "    # Compute centralities\n",
    "    centrality_dict = centralities(row['edgelist'])\n",
    "    \n",
    "    # Build a flat row per node\n",
    "    for vertex, (deg, clos, betw, pr) in centrality_dict.items():\n",
    "        rows.append({\n",
    "            'id': id,\n",
    "            'language': row['language'],\n",
    "            'sentence': row['sentence'],\n",
    "            'n': row['n'],\n",
    "            'vertex': vertex,\n",
    "            'degree': deg,\n",
    "            'closeness': clos,\n",
    "            'betweenness': betw,\n",
    "            'pagerank': pr,\n",
    "            'is_root': int(vertex == row['root'])\n",
    "        })\n",
    "\n",
    "# Convert list of rows to a DataFrame\n",
    "df_filtered = pd.DataFrame(rows)\n",
    "\n",
    "print(df_filtered.head)\n",
    "df_filtered.to_csv(\"./data/preprocessed/data_preprocessed.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111e2582",
   "metadata": {},
   "source": [
    "### Data normalization\n",
    "\n",
    "Different metrics are normalized at a Language sentence level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0c15211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Min-max normalization function\n",
    "def min_max_normalize(metric_dict):\n",
    "    values = np.array(list(metric_dict.values()), dtype=np.float64)\n",
    "    min_val = np.min(values)\n",
    "    max_val = np.max(values)\n",
    "    if max_val == min_val:\n",
    "        return {k: 0.0 for k in metric_dict}  # Avoid division by zero\n",
    "    return {k: (v - min_val) / (max_val - min_val) for k, v in metric_dict.items()}\n",
    "\n",
    "# Normalize centralities per sentence-language pair\n",
    "def normalize_centralities(df):\n",
    "    required_columns = ['sentence', 'language', 'vertex', 'degree', 'closeness', 'betweenness', 'pagerank', 'is_root']\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        raise ValueError(f\"Missing required columns: {set(required_columns) - set(df.columns)}\")\n",
    "    \n",
    "    metrics = ['n', 'degree', 'closeness', 'betweenness', 'pagerank']\n",
    "    result_frames = []\n",
    "\n",
    "    # Group by sentence and language\n",
    "    for (sentence, language), group in df.groupby(['sentence', 'language']):\n",
    "        norm_data = {}\n",
    "\n",
    "        for metric in metrics:\n",
    "            metric_dict = dict(zip(group['vertex'], group[metric]))\n",
    "            norm_data[metric] = min_max_normalize(metric_dict)\n",
    "\n",
    "        # Copy group and apply normalized values\n",
    "        norm_df = group.copy()\n",
    "        for metric in metrics:\n",
    "            norm_df[f'{metric}_norm'] = norm_df['vertex'].map(norm_data[metric])\n",
    "\n",
    "        result_frames.append(norm_df)\n",
    "\n",
    "    # Combine all normalized groups\n",
    "    return pd.concat(result_frames, ignore_index=True)\n",
    "\n",
    "# Apply normalization and save\n",
    "df_normalized = normalize_centralities(df_filtered)\n",
    "df_normalized.to_csv(\"./data/preprocessed/data_normalized.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
