{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1b20b4a",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "In this phase it would be developed the different models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a02a66",
   "metadata": {},
   "source": [
    "## Linear Classification\n",
    "According to the exploration of the boxplots the roots may have higher values in all of the metrics.\n",
    "The baseline solution proposed would be generating a simple program that selects the node with higher values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "444c855a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1 Results:\n",
      "  Train - Accuracy: 0.2746\n",
      "  Test  - Accuracy: 0.2571\n",
      "\n",
      "Fold 2 Results:\n",
      "  Train - Accuracy: 0.2689\n",
      "  Test  - Accuracy: 0.2786\n",
      "\n",
      "Fold 3 Results:\n",
      "  Train - Accuracy: 0.2668\n",
      "  Test  - Accuracy: 0.2886\n",
      "\n",
      "Fold 4 Results:\n",
      "  Train - Accuracy: 0.2717\n",
      "  Test  - Accuracy: 0.2767\n",
      "\n",
      "Fold 5 Results:\n",
      "  Train - Accuracy: 0.2742\n",
      "  Test  - Accuracy: 0.2552\n",
      "\n",
      "Final Model Coefficients (from last fold):\n",
      "  n_norm: -0.1137\n",
      "  degree_norm: 0.0209\n",
      "  closeness_norm: 0.0333\n",
      "  betweenness_norm: 0.1122\n",
      "  pagerank_norm: 0.0474\n",
      "Intercept: 0.0199\n",
      "\n",
      "=== Average Metrics Across Folds ===\n",
      "\n",
      "Train:\n",
      "Accuracy    0.271238\n",
      "dtype: float64\n",
      "\n",
      "Test:\n",
      "Accuracy    0.271238\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "def get_top_vertex_metrics(df, y_pred, target='is_root'):\n",
    "    \"\"\"\n",
    "    For each (sentence, language), select the vertex with the highest predicted score.\n",
    "    Compare it with the actual root vertex (where is_root == 1) and return match accuracy.\n",
    "    \n",
    "    Returns:\n",
    "        accuracy (float): Proportion of correct root predictions.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['y_pred'] = y_pred\n",
    "\n",
    "    # Group by sentence and language\n",
    "    groups = df.groupby(['sentence', 'language'])\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for (sentence, language), group in groups:\n",
    "        # Get actual root vertex\n",
    "        true_root_row = group[group[target] == 1]\n",
    "        if true_root_row.empty:\n",
    "            continue  # skip if no root in this group\n",
    "\n",
    "        true_vertex = true_root_row['vertex'].values[0]\n",
    "\n",
    "        # Get predicted top vertex (highest y_pred)\n",
    "        predicted_vertex = group.loc[group['y_pred'].idxmax(), 'vertex']\n",
    "\n",
    "        if predicted_vertex == true_vertex:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "def extract_features(df, features):\n",
    "    if 'avg_metrics' in features and 'avg_metrics' not in df.columns:\n",
    "        df['avg_metrics'] = df[['degree_norm', 'closeness_norm', 'betweenness_norm', 'pagerank_norm']].mean(axis=1)\n",
    "    return df[features].values\n",
    "\n",
    "\n",
    "def test_linear_model(folds_path, features, target, num_folds=5):\n",
    "    model = LinearRegression()\n",
    "    metrics = {\n",
    "        'train': pd.DataFrame(columns=['Accuracy']),\n",
    "        'test': pd.DataFrame(columns=['Accuracy'])\n",
    "    }\n",
    "\n",
    "    for fold in range(1, num_folds + 1):\n",
    "        train_path = os.path.join(folds_path, f'fold_{fold}_train.csv')\n",
    "        test_path = os.path.join(folds_path, f'fold_{fold}_test.csv')\n",
    "\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "\n",
    "        # Train model\n",
    "        X_train = extract_features(train_df, features)\n",
    "        y_train = train_df[target].values\n",
    "        X_test = extract_features(test_df, features)\n",
    "        y_test = test_df[target].values\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "\n",
    "        # Evaluate top-vertex prediction\n",
    "        train_scores = get_top_vertex_metrics(train_df, y_pred_train, target)\n",
    "        test_scores = get_top_vertex_metrics(test_df, y_pred_test, target)\n",
    "\n",
    "        metrics['train'].loc[f'Fold {fold}'] = [train_scores]\n",
    "        metrics['test'].loc[f'Fold {fold}'] = [test_scores]\n",
    "\n",
    "        print(f\"\\nFold {fold} Results:\")\n",
    "        print(f\"  Train - Accuracy: {train_scores:.4f}\")\n",
    "        print(f\"  Test  - Accuracy: {test_scores:.4f}\")\n",
    "\n",
    "    # Coefficients\n",
    "    print(\"\\nFinal Model Coefficients (from last fold):\")\n",
    "    for feature, coef in zip(features, model.coef_):\n",
    "        print(f\"  {feature}: {coef:.4f}\")\n",
    "    print(f\"Intercept: {model.intercept_:.4f}\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Example usage\n",
    "features = ['n_norm', 'degree_norm', 'closeness_norm', 'betweenness_norm', 'pagerank_norm']\n",
    "target = 'is_root'\n",
    "folds_path = './data/cross_validation'\n",
    "\n",
    "results_v1_dict = test_linear_model(folds_path, features, target)\n",
    "\n",
    "# Average metrics summary\n",
    "print(\"\\n=== Average Metrics Across Folds ===\")\n",
    "print(\"\\nTrain:\")\n",
    "print(results_v1_dict['train'].mean())\n",
    "print(\"\\nTest:\")\n",
    "print(results_v1_dict['test'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e620d6e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Fold 1</th>\n",
       "      <th>Fold 2</th>\n",
       "      <th>Fold 3</th>\n",
       "      <th>Fold 4</th>\n",
       "      <th>Fold 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LM_v1</td>\n",
       "      <td>0.271238</td>\n",
       "      <td>0.257143</td>\n",
       "      <td>0.278571</td>\n",
       "      <td>0.288571</td>\n",
       "      <td>0.276667</td>\n",
       "      <td>0.255238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model      Mean    Fold 1    Fold 2    Fold 3    Fold 4    Fold 5\n",
       "0  LM_v1  0.271238  0.257143  0.278571  0.288571  0.276667  0.255238"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_results(results, model_name):\n",
    "    # Copy and prepare test results\n",
    "    test_df = results['test'].copy()\n",
    "\n",
    "    # Calculate mean and insert as second row\n",
    "    mean_row = pd.DataFrame(test_df.mean()).T\n",
    "    mean_row.index = ['Mean']\n",
    "\n",
    "    # Add model name as the first row\n",
    "    model_row = pd.DataFrame([[model_name]], columns=test_df.columns, index=['Model'])\n",
    "\n",
    "    # Concatenate rows in desired order\n",
    "    test_df = pd.concat([model_row, mean_row, test_df])\n",
    "\n",
    "    # Transpose the DataFrame\n",
    "    test_df_results = test_df.T\n",
    "    test_df_results.index.values[0] = 'Metric'  # Optional: name first column\n",
    "    test_df_results['Model'] = model_name       # Add model name as a column\n",
    "    return test_df_results\n",
    "\n",
    "\n",
    "# Prepare both models\n",
    "results_v1 = prepare_results(results_v1_dict, \"LM_v1\")\n",
    "#results_v2 = prepare_results(results_v2_dict, \"LM_v2\")\n",
    "\n",
    "# Concatenate vertically\n",
    "combined_results = pd.concat([results_v1], ignore_index=True)\n",
    "\n",
    "# Save to CSV\n",
    "combined_results.to_csv(\"./data/models/all_models.csv\", index=False)\n",
    "\n",
    "# Display\n",
    "combined_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7360c74",
   "metadata": {},
   "source": [
    "### Linear Model with Feature Engineering\n",
    "\n",
    "In that model we have applied the linear model, but with a new variable the average betweeen the metrics extracted from the sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "94c5167c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1 Results:\n",
      "  Train - Accuracy: 0.2745\n",
      "  Test  - Accuracy: 0.2562\n",
      "\n",
      "Fold 2 Results:\n",
      "  Train - Accuracy: 0.2676\n",
      "  Test  - Accuracy: 0.2786\n",
      "\n",
      "Fold 3 Results:\n",
      "  Train - Accuracy: 0.2657\n",
      "  Test  - Accuracy: 0.2871\n",
      "\n",
      "Fold 4 Results:\n",
      "  Train - Accuracy: 0.2707\n",
      "  Test  - Accuracy: 0.2762\n",
      "\n",
      "Fold 5 Results:\n",
      "  Train - Accuracy: 0.2752\n",
      "  Test  - Accuracy: 0.2557\n",
      "\n",
      "Final Model Coefficients (from last fold):\n",
      "  n_norm: -0.1137\n",
      "  degree_norm: 0.0102\n",
      "  closeness_norm: 0.0226\n",
      "  betweenness_norm: 0.1015\n",
      "  pagerank_norm: 0.0367\n",
      "  avg_metrics: 0.0427\n",
      "Intercept: 0.0199\n",
      "\n",
      "=== Average Metrics Across Folds ===\n",
      "\n",
      "Train:\n",
      "Accuracy    0.270762\n",
      "dtype: float64\n",
      "\n",
      "Test:\n",
      "Accuracy    0.270762\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "features = ['n_norm', 'degree_norm', 'closeness_norm', 'betweenness_norm', 'pagerank_norm', 'avg_metrics']\n",
    "target = 'is_root'\n",
    "folds_path = './data/cross_validation'\n",
    "\n",
    "results_v2_dict = test_linear_model(folds_path, features, target)\n",
    "\n",
    "# Average metrics summary\n",
    "print(\"\\n=== Average Metrics Across Folds ===\")\n",
    "print(\"\\nTrain:\")\n",
    "print(results_v2_dict['train'].mean())\n",
    "print(\"\\nTest:\")\n",
    "print(results_v2_dict['test'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c333132f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Fold 1</th>\n",
       "      <th>Fold 2</th>\n",
       "      <th>Fold 3</th>\n",
       "      <th>Fold 4</th>\n",
       "      <th>Fold 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LM_v1</td>\n",
       "      <td>0.271238</td>\n",
       "      <td>0.257143</td>\n",
       "      <td>0.278571</td>\n",
       "      <td>0.288571</td>\n",
       "      <td>0.276667</td>\n",
       "      <td>0.255238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LM_v2</td>\n",
       "      <td>0.270762</td>\n",
       "      <td>0.25619</td>\n",
       "      <td>0.278571</td>\n",
       "      <td>0.287143</td>\n",
       "      <td>0.27619</td>\n",
       "      <td>0.255714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model      Mean    Fold 1    Fold 2    Fold 3    Fold 4    Fold 5\n",
       "0  LM_v1  0.271238  0.257143  0.278571  0.288571  0.276667  0.255238\n",
       "1  LM_v2  0.270762   0.25619  0.278571  0.287143   0.27619  0.255714"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_v2 = prepare_results(results_v2_dict, \"LM_v2\")\n",
    "\n",
    "# Concatenate vertically\n",
    "combined_results = pd.concat([results_v1, results_v2], ignore_index=True)\n",
    "\n",
    "# Save to CSV\n",
    "combined_results.to_csv(\"./data/models/all_models.csv\", index=False)\n",
    "\n",
    "# Sort by the average of the accuracy\n",
    "sorted_df = combined_results.sort_values(by='Mean', ascending=False).T\n",
    "\n",
    "# Display\n",
    "sorted_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b10c6e0",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027f690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a code that picks the best "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
