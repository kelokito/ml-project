{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1309631b",
   "metadata": {},
   "source": [
    "# Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60f6b8e",
   "metadata": {},
   "source": [
    "## Preprocessing Test Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe42a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe columns\n",
      "Index(['id', 'language', 'sentence', 'n', 'edgelist'], dtype='object')\n",
      "Dataframe rows\n",
      "10395\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "df = pd.read_csv(\"./data/raw_files/test.csv\")\n",
    "df['edgelist'] = df['edgelist'].apply(ast.literal_eval) # to verify that we are sending a list instead of an array\n",
    "\n",
    "print(\"Dataframe columns\")\n",
    "print(df.columns)\n",
    "\n",
    "print(\"Dataframe rows\")\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74212600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def centralities(edgelist):\n",
    "    \"\"\"\n",
    "    - edgelist is a list of node pairs e.g. [(7,2),(1,7),(1,9),...]\n",
    "    - returns a dictionary of vertex -> (centrality values)\n",
    "    \"\"\"\n",
    "    T = nx.from_edgelist(edgelist)\n",
    "    dc = nx.degree_centrality(T)\n",
    "    cc = nx.harmonic_centrality(T)\n",
    "    bc = nx.betweenness_centrality(T)\n",
    "    pc = nx.pagerank(T)\n",
    "    katz = nx.katz_centrality_numpy(T)\n",
    "    load = nx.load_centrality(T)\n",
    "\n",
    "    return {v: (dc[v], cc[v], bc[v], pc[v], katz[v], load[v]) for v in T}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55d8366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  language  sentence   n  vertex    degree  closeness  betweenness  \\\n",
      "0   1  Japanese         1  43      38  0.047619   8.953882     0.047619   \n",
      "1   1  Japanese         1  43      33  0.023810   7.094756     0.000000   \n",
      "2   1  Japanese         1  43      10  0.095238  11.348363     0.335656   \n",
      "3   1  Japanese         1  43      24  0.047619   6.855212     0.047619   \n",
      "4   1  Japanese         1  43      16  0.023810   5.649594     0.000000   \n",
      "\n",
      "   pagerank      katz      load  \n",
      "0  0.024647  0.152204  0.047619  \n",
      "1  0.013964  0.135660  0.000000  \n",
      "2  0.043718  0.181985  0.335656  \n",
      "3  0.026723  0.149016  0.047619  \n",
      "4  0.014845  0.135341  0.000000  \n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    # Compute centralities\n",
    "    centrality_dict = centralities(row['edgelist'])\n",
    "    \n",
    "    for vertex, (deg, clos, betw, pr, katz, load) in centrality_dict.items():\n",
    "        rows.append({\n",
    "            'id': row['id'],\n",
    "            'language': row['language'],\n",
    "            'sentence': row['sentence'],\n",
    "            'n': row['n'],\n",
    "            'vertex': vertex,\n",
    "            'degree': deg,\n",
    "            'closeness': clos,\n",
    "            'betweenness': betw,\n",
    "            'pagerank': pr,\n",
    "            'katz': katz,\n",
    "            'load': load\n",
    "        })\n",
    "\n",
    "df_filtered = pd.DataFrame(rows)\n",
    "\n",
    "print(df_filtered.head())\n",
    "df_filtered.to_csv(\"./data/preprocessed/test/test_preprocessed.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b32caf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Min-max normalization function\n",
    "def min_max_normalize(metric_dict):\n",
    "    values = np.array(list(metric_dict.values()), dtype=np.float64)\n",
    "    min_val = np.min(values)\n",
    "    max_val = np.max(values)\n",
    "    if max_val == min_val:\n",
    "        return {k: 0.0 for k in metric_dict}  # Avoid division by zero\n",
    "    return {k: (v - min_val) / (max_val - min_val) for k, v in metric_dict.items()}\n",
    "\n",
    "# Normalize centralities per sentence-language pair\n",
    "def normalize_centralities(df):\n",
    "    required_columns = ['id', 'sentence', 'language', 'vertex', 'degree', 'closeness', 'betweenness', 'pagerank', 'katz', 'load']\n",
    "\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        raise ValueError(f\"Missing required columns: {set(required_columns) - set(df.columns)}\")\n",
    "    \n",
    "    metrics = metrics = ['degree', 'closeness', 'betweenness', 'pagerank', 'katz', 'load']\n",
    "\n",
    "    result_frames = []\n",
    "\n",
    "    # Group by sentence and language\n",
    "    for (sentence, language), group in df.groupby(['sentence', 'language']):\n",
    "        norm_data = {}\n",
    "\n",
    "        for metric in metrics:\n",
    "            metric_dict = dict(zip(group['vertex'], group[metric]))\n",
    "            norm_data[metric] = min_max_normalize(metric_dict)\n",
    "\n",
    "        # Copy group and apply normalized values\n",
    "        norm_df = group.copy()\n",
    "        for metric in metrics:\n",
    "            norm_df[f'{metric}_norm'] = norm_df['vertex'].map(norm_data[metric])\n",
    "\n",
    "        result_frames.append(norm_df)\n",
    "\n",
    "    # Combine all normalized groups\n",
    "    return pd.concat(result_frames, ignore_index=True)\n",
    "\n",
    "# Normalize length \n",
    "def normalize_length(df):\n",
    "    df['n_norm'] = (df['n'] - df['n'].min()) / (df['n'].max() - df['n'].min())\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply normalization and save\n",
    "df_normalized = normalize_centralities(df_filtered)\n",
    "\n",
    "df_normalized = normalize_length(df_normalized)\n",
    "\n",
    "df_normalized.to_csv(\"./data/preprocessed/test/test_normalized.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914cd9b3",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0927105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def get_top_vertex_metrics(df, y_pred, target='is_root'):\n",
    "    \"\"\"\n",
    "    For each (sentence, language), select the vertex with the highest predicted score.\n",
    "    Compare it with the actual root vertex (where is_root == 1) and return match accuracy.\n",
    "    \n",
    "    Returns:\n",
    "        accuracy (float): Proportion of correct root predictions.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['y_pred'] = y_pred\n",
    "\n",
    "    # Group by sentence and language\n",
    "    groups = df.groupby(['sentence', 'language'])\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for (sentence, language), group in groups:\n",
    "        # Get actual root vertex\n",
    "        true_root_row = group[group[target] == 1]\n",
    "        if true_root_row.empty:\n",
    "            continue  # skip if no root in this group\n",
    "\n",
    "        true_vertex = true_root_row['vertex'].values[0]\n",
    "\n",
    "        # Get predicted top vertex (highest y_pred)\n",
    "        predicted_vertex = group.loc[group['y_pred'].idxmax(), 'vertex']\n",
    "\n",
    "        if predicted_vertex == true_vertex:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "def extract_features(df, features):\n",
    "    # Create 'avg_metrics' if needed\n",
    "    if 'avg_metrics' in features and 'avg_metrics' not in df.columns:\n",
    "        df['avg_metrics'] = df[['degree_norm', 'closeness_norm', 'betweenness_norm', 'pagerank_norm', 'katz_norm', 'load_norm']].mean(axis=1)\n",
    "    \n",
    "    # One-hot encode 'language' (if present)\n",
    "    if 'language' in features:\n",
    "        df = pd.get_dummies(df, columns=['language'], drop_first=True)\n",
    "        # Update features list to include new dummy columns\n",
    "        new_language_features = [col for col in df.columns if col.startswith('language_')]\n",
    "        features = [f for f in features if f != 'language'] + new_language_features\n",
    "    \n",
    "    return df[features].values\n",
    "    \n",
    "def get_test_results(df, y_pred):\n",
    "    \"\"\"\n",
    "    Return a DataFrame with one row per sentence ID, showing the predicted root vertex.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['y_pred'] = y_pred\n",
    "\n",
    "    results = []\n",
    "    for id_val, group in df.groupby('id'):\n",
    "        top_vertex = group.loc[group['y_pred'].idxmax()]\n",
    "        results.append({\n",
    "            'id': int(id_val),\n",
    "            'root': int(top_vertex['vertex'])\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def test_linear_model(model, features, target):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a model. Returns test predictions (root vertex per sentence-language group).\n",
    "    \"\"\"\n",
    "    train_path = './data/preprocessed/data_normalized.csv'\n",
    "    test_path = './data/preprocessed/test/test_normalized.csv'\n",
    "\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "\n",
    "    # Train\n",
    "    X_train = extract_features(train_df, features)\n",
    "    y_train = train_df[target].values\n",
    "    X_test = extract_features(test_df, features)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_proba_train = model.predict_proba(X_train)[:, 1]\n",
    "    y_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Accuracy (top vertex match)\n",
    "    train_accuracy = get_top_vertex_metrics(train_df, y_proba_train, target)\n",
    "    print(f\"Train - Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "    # Return test root predictions\n",
    "    test_results = get_test_results(test_df, y_proba_test)\n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b742a2",
   "metadata": {},
   "source": [
    "## Logistic Regression Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "959081ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Accuracy: 0.2717\n",
      "   id  root\n",
      "0   1    37\n",
      "1   2    46\n",
      "2   3     2\n",
      "3   4    11\n",
      "4   5     3\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "features = ['degree_norm', 'closeness_norm', 'betweenness_norm', 'pagerank_norm', 'katz_norm', 'load_norm']\n",
    "target = 'is_root'\n",
    "\n",
    "test_predictions = test_linear_model(model, features, target)\n",
    "test_predictions.to_csv(f'./data/results/LogReg_V1.csv', index=False)\n",
    "print(test_predictions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3729d3ed",
   "metadata": {},
   "source": [
    "## Random Forest Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fc3f27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Accuracy: 0.8521\n",
      "   id  root\n",
      "0   1     2\n",
      "1   2    15\n",
      "2   3     2\n",
      "3   4     5\n",
      "4   5     1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "features = ['degree_norm', 'closeness_norm', 'betweenness_norm', 'pagerank_norm', 'katz_norm', 'load_norm']\n",
    "target = 'is_root'\n",
    "\n",
    "test_predictions = test_linear_model(model, features, target)\n",
    "test_predictions.to_csv(f'./data/results/RF_V4.csv', index=False)\n",
    "print(test_predictions.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f2839a",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07a1f1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Accuracy: 0.2893\n",
      "   id  root\n",
      "0   1    37\n",
      "1   2    46\n",
      "2   3     2\n",
      "3   4    11\n",
      "4   5     3\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "model = XGBClassifier(n_estimators=100,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,  # L1 regularization\n",
    "    reg_lambda=1.0, # L2 regularization\n",
    "    eval_metric='logloss',\n",
    "    random_state=42)\n",
    "features = ['degree_norm', 'closeness_norm', 'betweenness_norm', 'pagerank_norm', 'katz_norm', 'load_norm']\n",
    "target = 'is_root'\n",
    "\n",
    "test_predictions = test_linear_model(model, features, target)\n",
    "test_predictions.to_csv(f'./data/results/XGB.csv', index=False)\n",
    "print(test_predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5551b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabri\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
      "  warnings.warn(\n",
      "c:\\Users\\gabri\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Accuracy: 0.2435\n",
      "   id  root\n",
      "0   1    25\n",
      "1   2    24\n",
      "2   3    11\n",
      "3   4    11\n",
      "4   5     1\n"
     ]
    }
   ],
   "source": [
    "model = QuadraticDiscriminantAnalysis()\n",
    "features = ['degree_norm', 'closeness_norm', 'betweenness_norm', 'pagerank_norm', 'katz_norm', 'load_norm']\n",
    "target = 'is_root'\n",
    "\n",
    "test_predictions = test_linear_model(model, features, target)\n",
    "test_predictions.to_csv(f'./data/results/QDA.csv', index=False)\n",
    "print(test_predictions.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0239829",
   "metadata": {},
   "source": [
    "## Best models by language (with train data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea85ecae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best models per language saved to: ./data/models/best_model_per_language.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>best_model</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Chinese</td>\n",
       "      <td>LogReg_V1</td>\n",
       "      <td>0.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>RF_V3</td>\n",
       "      <td>0.310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>German</td>\n",
       "      <td>RF_V3</td>\n",
       "      <td>0.288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Galician</td>\n",
       "      <td>RF_V3</td>\n",
       "      <td>0.306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Italian</td>\n",
       "      <td>RF_V3</td>\n",
       "      <td>0.262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    language best_model  accuracy\n",
       "16   Chinese  LogReg_V1     0.290\n",
       "0     Arabic      RF_V3     0.310\n",
       "18    German      RF_V3     0.288\n",
       "17  Galician      RF_V3     0.306\n",
       "15   Italian      RF_V3     0.262"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executed_models = [f.split('.')[0] for f in os.listdir('./data/results') if f.endswith('.csv')]\n",
    "\n",
    "models_metrics = models_metrics[models_metrics['model'].isin(executed_models)]\n",
    "\n",
    "\n",
    "exclude_columns = ['model', 'Train Accuracy', 'Test Accuracy']\n",
    "language_columns = [col for col in models_metrics.columns if col not in exclude_columns]\n",
    "\n",
    "results = []\n",
    "for language in language_columns:\n",
    "    best_idx = models_metrics[language].idxmax()\n",
    "    best_model = models_metrics.loc[best_idx, 'model']\n",
    "    best_accuracy = models_metrics.loc[best_idx, language]\n",
    "    results.append({'language': language, 'best_model': best_model, 'accuracy': best_accuracy})\n",
    "\n",
    "\n",
    "best_models_df = pd.DataFrame(results)\n",
    "\n",
    "best_models_df = best_models_df.sort_values(by='best_model')\n",
    "\n",
    "\n",
    "best_models_df.to_csv('./data/models/best_model_per_language.csv', index=False)\n",
    "print(\"Best models per language saved to: ./data/models/best_model_per_language.csv\")\n",
    "best_models_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff499fb5",
   "metadata": {},
   "source": [
    "## All models tested\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3df92bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LinReg_V1.csv — Accuracy: 0.2977 (3095/10395)\n",
      "\n",
      "Model: LogReg_V1.csv — Accuracy: 0.2933 (3049/10395)\n",
      "\n",
      "Model: QDA.csv — Accuracy: 0.2618 (2721/10395)\n",
      "\n",
      "Model: RF_V3.csv — Accuracy: 0.2516 (2615/10395)\n",
      "\n",
      "Model: RF_V4.csv — Accuracy: 0.2567 (2668/10395)\n",
      "\n",
      "Model: XGB.csv — Accuracy: 0.3135 (3259/10395)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path where your model predictions are stored\n",
    "test_file_path = './data/results'\n",
    "\n",
    "# Load the ground truth submission file\n",
    "final_results = pd.read_csv('./data/tests/results.csv')\n",
    "\n",
    "# Loop through all files in the results directory\n",
    "for model_file in os.listdir(test_file_path):\n",
    "    if not model_file.endswith('.csv'):\n",
    "        continue  # skip non-CSV files\n",
    "\n",
    "    model_path = os.path.join(test_file_path, model_file)\n",
    "    model_df = pd.read_csv(model_path)\n",
    "\n",
    "    # Ensure consistent formatting\n",
    "    merged = final_results.merge(model_df, on='id', suffixes=('_true', '_pred'))\n",
    "\n",
    "    # Count matches\n",
    "    correct = (merged['root'] == merged['leaked_root']).sum()\n",
    "    accuracy = correct / len(merged)\n",
    "\n",
    "    print(f\"Model: {model_file} — Accuracy: {accuracy:.4f} ({correct}/{len(merged)})\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
