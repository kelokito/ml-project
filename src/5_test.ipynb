{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1309631b",
   "metadata": {},
   "source": [
    "# Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60f6b8e",
   "metadata": {},
   "source": [
    "## Preprocessing Test Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fe42a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe columns\n",
      "Index(['id', 'language', 'sentence', 'n', 'edgelist'], dtype='object')\n",
      "Dataframe rows\n",
      "10395\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"./data/raw_files/test.csv\")\n",
    "df['edgelist'] = df['edgelist'].apply(ast.literal_eval) # to verify that we are sending a list instead of an array\n",
    "\n",
    "print(\"Dataframe columns\")\n",
    "print(df.columns)\n",
    "\n",
    "print(\"Dataframe rows\")\n",
    "print(len(df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74212600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def centralities(edgelist):\n",
    "    \"\"\"\n",
    "    - edgelist is a list of node pairs e.g. [(7,2),(1,7),(1,9),...]\n",
    "    - returns a dictionary of vertex -> (centrality values)\n",
    "    \"\"\"\n",
    "    T = nx.from_edgelist(edgelist)\n",
    "    dc = nx.degree_centrality(T)\n",
    "    cc = nx.harmonic_centrality(T)\n",
    "    bc = nx.betweenness_centrality(T)\n",
    "    pc = nx.pagerank(T)\n",
    "    return {v: (dc[v], cc[v], bc[v], pc[v]) for v in T}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e55d8366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of            id  language  sentence   n  vertex    degree  closeness  \\\n",
      "0           1  Japanese         1  43      38  0.047619   8.953882   \n",
      "1           1  Japanese         1  43      33  0.023810   7.094756   \n",
      "2           1  Japanese         1  43      10  0.095238  11.348363   \n",
      "3           1  Japanese         1  43      24  0.047619   6.855212   \n",
      "4           1  Japanese         1  43      16  0.023810   5.649594   \n",
      "...       ...       ...       ...  ..     ...       ...        ...   \n",
      "194643  10395   Russian       993  16       7  0.133333   6.266667   \n",
      "194644  10395   Russian       993  16       5  0.133333   5.292857   \n",
      "194645  10395   Russian       993  16      11  0.066667   4.070238   \n",
      "194646  10395   Russian       993  16       2  0.133333   6.016667   \n",
      "194647  10395   Russian       993  16       1  0.066667   4.492857   \n",
      "\n",
      "        betweenness  pagerank  \n",
      "0          0.047619  0.024647  \n",
      "1          0.000000  0.013964  \n",
      "2          0.335656  0.043718  \n",
      "3          0.047619  0.026723  \n",
      "4          0.000000  0.014845  \n",
      "...             ...       ...  \n",
      "194643     0.247619  0.065081  \n",
      "194644     0.133333  0.070454  \n",
      "194645     0.000000  0.039319  \n",
      "194646     0.133333  0.067486  \n",
      "194647     0.000000  0.038056  \n",
      "\n",
      "[194648 rows x 9 columns]>\n"
     ]
    }
   ],
   "source": [
    "# List to collect rows for the final DataFrame\n",
    "rows = []\n",
    "\n",
    "# Iterate over DataFrame rows\n",
    "for idx, row in df.iterrows():\n",
    "    # Compute centralities\n",
    "    centrality_dict = centralities(row['edgelist'])\n",
    "    \n",
    "    # Build a flat row per node\n",
    "    for vertex, (deg, clos, betw, pr) in centrality_dict.items():\n",
    "        rows.append({\n",
    "            'id': row['id'],\n",
    "            'language': row['language'],\n",
    "            'sentence': row['sentence'],\n",
    "            'n': row['n'],\n",
    "            'vertex': vertex,\n",
    "            'degree': deg,\n",
    "            'closeness': clos,\n",
    "            'betweenness': betw,\n",
    "            'pagerank': pr\n",
    "        })\n",
    "\n",
    "# Convert list of rows to a DataFrame\n",
    "df_filtered = pd.DataFrame(rows)\n",
    "\n",
    "print(df_filtered.head)\n",
    "df_filtered.to_csv(\"./data/preprocessed/test/test_preprocessed.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b32caf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Min-max normalization function\n",
    "def min_max_normalize(metric_dict):\n",
    "    values = np.array(list(metric_dict.values()), dtype=np.float64)\n",
    "    min_val = np.min(values)\n",
    "    max_val = np.max(values)\n",
    "    if max_val == min_val:\n",
    "        return {k: 0.0 for k in metric_dict}  # Avoid division by zero\n",
    "    return {k: (v - min_val) / (max_val - min_val) for k, v in metric_dict.items()}\n",
    "\n",
    "# Normalize centralities per sentence-language pair\n",
    "def normalize_centralities(df):\n",
    "    required_columns = ['id', 'sentence', 'language', 'vertex', 'degree', 'closeness', 'betweenness', 'pagerank']\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        raise ValueError(f\"Missing required columns: {set(required_columns) - set(df.columns)}\")\n",
    "    \n",
    "    metrics = ['degree', 'closeness', 'betweenness', 'pagerank']\n",
    "    result_frames = []\n",
    "\n",
    "    # Group by sentence and language\n",
    "    for (sentence, language), group in df.groupby(['sentence', 'language']):\n",
    "        norm_data = {}\n",
    "\n",
    "        for metric in metrics:\n",
    "            metric_dict = dict(zip(group['vertex'], group[metric]))\n",
    "            norm_data[metric] = min_max_normalize(metric_dict)\n",
    "\n",
    "        # Copy group and apply normalized values\n",
    "        norm_df = group.copy()\n",
    "        for metric in metrics:\n",
    "            norm_df[f'{metric}_norm'] = norm_df['vertex'].map(norm_data[metric])\n",
    "\n",
    "        result_frames.append(norm_df)\n",
    "\n",
    "    # Combine all normalized groups\n",
    "    return pd.concat(result_frames, ignore_index=True)\n",
    "\n",
    "# Normalize length \n",
    "def normalize_length(df):\n",
    "    df['n_norm'] = (df['n'] - df['n'].min()) / (df['n'].max() - df['n'].min())\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply normalization and save\n",
    "df_normalized = normalize_centralities(df_filtered)\n",
    "\n",
    "df_normalized = normalize_length(df_normalized)\n",
    "\n",
    "df_normalized.to_csv(\"./data/preprocessed/test/test_normalized.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914cd9b3",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0927105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "def get_top_vertex_metrics(df, y_pred, target='is_root'):\n",
    "    \"\"\"\n",
    "    For each (sentence, language), select the vertex with the highest predicted score.\n",
    "    Compare it with the actual root vertex (where is_root == 1) and return match accuracy.\n",
    "    \n",
    "    Returns:\n",
    "        accuracy (float): Proportion of correct root predictions.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['y_pred'] = y_pred\n",
    "\n",
    "    # Group by sentence and language\n",
    "    groups = df.groupby(['sentence', 'language'])\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for (sentence, language), group in groups:\n",
    "        # Get actual root vertex\n",
    "        true_root_row = group[group[target] == 1]\n",
    "        if true_root_row.empty:\n",
    "            continue  # skip if no root in this group\n",
    "\n",
    "        true_vertex = true_root_row['vertex'].values[0]\n",
    "\n",
    "        # Get predicted top vertex (highest y_pred)\n",
    "        predicted_vertex = group.loc[group['y_pred'].idxmax(), 'vertex']\n",
    "\n",
    "        if predicted_vertex == true_vertex:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "def extract_features(df, features):\n",
    "    # Create 'avg_metrics' if needed\n",
    "    if 'avg_metrics' in features and 'avg_metrics' not in df.columns:\n",
    "        df['avg_metrics'] = df[['degree_norm', 'closeness_norm', 'betweenness_norm', 'pagerank_norm']].mean(axis=1)\n",
    "    \n",
    "    # One-hot encode 'language' (if present)\n",
    "    if 'language' in features:\n",
    "        df = pd.get_dummies(df, columns=['language'], drop_first=True)\n",
    "        # Update features list to include new dummy columns\n",
    "        new_language_features = [col for col in df.columns if col.startswith('language_')]\n",
    "        features = [f for f in features if f != 'language'] + new_language_features\n",
    "    \n",
    "    return df[features].values\n",
    "    \n",
    "def get_test_results(df, y_pred):\n",
    "    \"\"\"\n",
    "    Return a DataFrame with one row per sentence ID, showing the predicted root vertex.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['y_pred'] = y_pred\n",
    "\n",
    "    results = []\n",
    "    for id_val, group in df.groupby('id'):\n",
    "        top_vertex = group.loc[group['y_pred'].idxmax()]\n",
    "        results.append({\n",
    "            'id': int(id_val),\n",
    "            'root': int(top_vertex['vertex'])\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def test_linear_model(model, features, target):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a model. Returns test predictions (root vertex per sentence-language group).\n",
    "    \"\"\"\n",
    "    train_path = './data/preprocessed/data_normalized.csv'\n",
    "    test_path = './data/preprocessed/test/test_normalized.csv'\n",
    "\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "\n",
    "    # Train\n",
    "    X_train = extract_features(train_df, features)\n",
    "    y_train = train_df[target].values\n",
    "    X_test = extract_features(test_df, features)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_proba_train = model.predict_proba(X_train)[:, 1]\n",
    "    y_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Accuracy (top vertex match)\n",
    "    train_accuracy = get_top_vertex_metrics(train_df, y_proba_train, target)\n",
    "    print(f\"Train - Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "    # Return test root predictions\n",
    "    test_results = get_test_results(test_df, y_proba_test)\n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7637d56",
   "metadata": {},
   "source": [
    "## Linear Regression Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bab7c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Accuracy: 0.2706\n",
      "   id  root\n",
      "0   1    37\n",
      "1   2    46\n",
      "2   3     2\n",
      "3   4    11\n",
      "4   5     3\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "features = ['n_norm', 'degree_norm', 'closeness_norm', 'betweenness_norm', 'pagerank_norm']\n",
    "target = 'is_root'\n",
    "\n",
    "test_predictions = test_linear_model(model, features, target)\n",
    "test_predictions.to_csv(f'./data/results/LinReg_V1.csv', index=False)\n",
    "print(test_predictions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b742a2",
   "metadata": {},
   "source": [
    "## Logistic Regression Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "959081ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Accuracy: 0.5890\n",
      "   id  root\n",
      "0   1    38\n",
      "1   2    17\n",
      "2   3    15\n",
      "3   4     6\n",
      "4   5     3\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "features = ['n_norm', 'degree_norm', 'closeness_norm', 'betweenness_norm', 'pagerank_norm']\n",
    "target = 'is_root'\n",
    "\n",
    "test_predictions = test_linear_model(model, features, target)\n",
    "test_predictions.to_csv(f'./data/results/LogReg_V1.csv', index=False)\n",
    "print(test_predictions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3729d3ed",
   "metadata": {},
   "source": [
    "## Random Forest Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fc3f27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Accuracy: 0.8509\n",
      "   id  root\n",
      "0   1     2\n",
      "1   2    22\n",
      "2   3    35\n",
      "3   4     5\n",
      "4   5     1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "features = ['n_norm', 'degree_norm', 'closeness_norm', 'betweenness_norm', 'pagerank_norm']\n",
    "target = 'is_root'\n",
    "\n",
    "test_predictions = test_linear_model(model, features, target)\n",
    "test_predictions.to_csv(f'./data/results/RF_V4.csv', index=False)\n",
    "print(test_predictions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f2839a",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07a1f1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Accuracy: 0.2929\n",
      "   id  root\n",
      "0   1    37\n",
      "1   2    46\n",
      "2   3     2\n",
      "3   4    11\n",
      "4   5     3\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "model = XGBClassifier(n_estimators=100,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,  # L1 regularization\n",
    "    reg_lambda=1.0, # L2 regularization\n",
    "    eval_metric='logloss',\n",
    "    random_state=42)\n",
    "features = ['n_norm', 'degree_norm', 'closeness_norm', 'betweenness_norm', 'pagerank_norm']\n",
    "target = 'is_root'\n",
    "\n",
    "test_predictions = test_linear_model(model, features, target)\n",
    "test_predictions.to_csv(f'./data/results/XGB.csv', index=False)\n",
    "print(test_predictions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0239829",
   "metadata": {},
   "source": [
    "## Hybrid Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea85ecae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best models per language saved to: ./data/models/best_model_per_language.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>best_model</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Chinese</td>\n",
       "      <td>LogReg_V1</td>\n",
       "      <td>0.288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>French</td>\n",
       "      <td>RF_V1</td>\n",
       "      <td>0.270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>German</td>\n",
       "      <td>RF_V2</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Galician</td>\n",
       "      <td>RF_V2</td>\n",
       "      <td>0.326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hindi</td>\n",
       "      <td>RF_V3</td>\n",
       "      <td>0.224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    language best_model  accuracy\n",
       "16   Chinese  LogReg_V1     0.288\n",
       "10    French      RF_V1     0.270\n",
       "18    German      RF_V2     0.302\n",
       "17  Galician      RF_V2     0.326\n",
       "2      Hindi      RF_V3     0.224"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "models_metrics = pd.read_csv('./data/models/all_models.csv')\n",
    "\n",
    "# Define columns to exclude\n",
    "exclude_columns = ['model', 'Train Accuracy', 'Test Accuracy']\n",
    "language_columns = [col for col in models_metrics.columns if col not in exclude_columns]\n",
    "\n",
    "# Collect best model and score per language\n",
    "results = []\n",
    "for language in language_columns:\n",
    "    best_idx = models_metrics[language].idxmax()\n",
    "    best_model = models_metrics.loc[best_idx, 'model']\n",
    "    best_accuracy = models_metrics.loc[best_idx, language]\n",
    "    results.append({'language': language, 'best_model': best_model, 'accuracy': best_accuracy})\n",
    "\n",
    "# Convert to DataFrame\n",
    "best_models_df = pd.DataFrame(results)\n",
    "\n",
    "# Sort by best_model (optional)\n",
    "best_models_df = best_models_df.sort_values(by='best_model')\n",
    "\n",
    "# Save and display\n",
    "best_models_df.to_csv('./data/models/best_model_per_language.csv', index=False)\n",
    "print(\"✅ Best models per language saved to: ./data/models/best_model_per_language.csv\")\n",
    "best_models_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1697f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff499fb5",
   "metadata": {},
   "source": [
    "## All models tested\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3df92bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LinReg_V1.csv — Accuracy: 0.2977 (3095/10395)\n",
      "\n",
      "Model: LogReg_V1.csv — Accuracy: 0.6016 (6254/10395)\n",
      "\n",
      "Model: RF_V3.csv — Accuracy: 0.2516 (2615/10395)\n",
      "\n",
      "Model: RF_V4.csv — Accuracy: 0.3005 (3124/10395)\n",
      "\n",
      "Model: XGB.csv — Accuracy: 0.3151 (3275/10395)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path where your model predictions are stored\n",
    "test_file_path = './data/results'\n",
    "\n",
    "# Load the ground truth submission file\n",
    "final_results = pd.read_csv('./data/tests/results.csv')\n",
    "\n",
    "# Loop through all files in the results directory\n",
    "for model_file in os.listdir(test_file_path):\n",
    "    if not model_file.endswith('.csv'):\n",
    "        continue  # skip non-CSV files\n",
    "\n",
    "    model_path = os.path.join(test_file_path, model_file)\n",
    "    model_df = pd.read_csv(model_path)\n",
    "\n",
    "    # Ensure consistent formatting\n",
    "    merged = final_results.merge(model_df, on='id', suffixes=('_true', '_pred'))\n",
    "\n",
    "    # Count matches\n",
    "    correct = (merged['root'] == merged['leaked_root']).sum()\n",
    "    accuracy = correct / len(merged)\n",
    "\n",
    "    print(f\"Model: {model_file} — Accuracy: {accuracy:.4f} ({correct}/{len(merged)})\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
